# Mini Data Lake ğŸš€

Un data lake minimaliste avec **Apache Spark** pour gÃ©rer le cycle de vie des donnÃ©es : ingestion, transformation et curation.

## ğŸ“ Structure

```
mini-data-lake/
â”œâ”€â”€ data_lake/
â”‚   â”œâ”€â”€ raw/                    # DonnÃ©es brutes (CSV, JSON)
â”‚   â”‚   â”œâ”€â”€ transactions/
â”‚   â”‚   â”œâ”€â”€ sales/
â”‚   â”‚   â””â”€â”€ logs/
â”‚   â”œâ”€â”€ staging/                # DonnÃ©es transformÃ©es (Parquet)
â”‚   â””â”€â”€ curated/                # DonnÃ©es agrÃ©gÃ©es (Parquet/JSON)
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ spark_config.py         # Configuration Spark
â”‚   â”œâ”€â”€ spark_ingest.py         # Job d'ingestion
â”‚   â”œâ”€â”€ spark_transform.py      # Job de transformation
â”‚   â”œâ”€â”€ spark_curate.py         # Job de curation
â”‚   â”œâ”€â”€ spark_pipeline.py       # Pipeline complet
â”‚   â”œâ”€â”€ ingest.py               # (Python standard)
â”‚   â”œâ”€â”€ transform.py            # (Python standard)
â”‚   â””â”€â”€ curate.py               # (Python standard)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš¡ Installation

```bash
# CrÃ©er un environnement virtuel
python -m venv venv 
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer PySpark
pip install -r requirements.txt
```

> **Note**: Spark nÃ©cessite Java 8/11/17. VÃ©rifiez avec `java -version`.

## ğŸ”„ Pipeline Spark

### ExÃ©cuter le pipeline complet

```bash
cd jobs
python spark_pipeline.py
```

Ce pipeline orchestre automatiquement :
1. **Ingest** â†’ Lecture des fichiers raw (CSV/JSON)
2. **Transform** â†’ Nettoyage + enrichissement â†’ staging (Parquet)
3. **Curate** â†’ AgrÃ©gations + mÃ©triques â†’ curated

### ExÃ©cuter les jobs individuellement

```bash
# Ingestion uniquement
python spark_ingest.py

# Transformation uniquement
python spark_transform.py

# Curation uniquement
python spark_curate.py
```

## ğŸ“Š FonctionnalitÃ©s Spark

### Transformations

```python
from spark_transform import transform_category

# Transforme et enrichit les logs
df = transform_category(spark, "logs")
# Ajoute: date, hour, day_of_week, is_success, ip_last_octet...
```

### MÃ©triques gÃ©nÃ©rÃ©es (Curation)

| Dataset | Description |
|---------|-------------|
| `global_metrics` | Statistiques globales (total, succÃ¨s, Ã©checs, taux) |
| `user_statistics` | Stats par utilisateur avec ranking |
| `hourly_statistics` | Distribution horaire des connexions |
| `suspicious_ips` | IPs avec taux d'Ã©chec Ã©levÃ© |

### Exemple de sortie

```
ğŸ“Š MÃ‰TRIQUES GLOBALES
+---------------+------------------+--------------+--------------+------------+
|total_attempts |successful_logins |failed_logins |unique_users  |success_rate|
+---------------+------------------+--------------+--------------+------------+
|400            |180               |220           |95            |45.0        |
+---------------+------------------+--------------+--------------+------------+
```

## ğŸ”§ Configuration Spark

Le fichier `spark_config.py` configure :

```python
spark = SparkSession.builder \
    .appName("MiniDataLake") \
    .master("local[*]")           # Utilise tous les cores
    .config("spark.driver.memory", "2g")
    .getOrCreate()
```

## ğŸ“¦ Formats de donnÃ©es

| Zone | Format | Raison |
|------|--------|--------|
| **Raw** | CSV, JSON | Formats source originaux |
| **Staging** | Parquet | Columnar, compressÃ©, typÃ© |
| **Curated** | Parquet/JSON | OptimisÃ© pour l'analyse |

## ğŸ› ï¸ Jobs Python Standard

Les jobs Python standard (sans Spark) restent disponibles pour les petits volumes :

```bash
python ingest.py
python transform.py
python curate.py
```

## ğŸ“ˆ Cas d'usage

Ce mini data lake est conÃ§u pour :
- âœ… Apprentissage de Spark
- âœ… Prototypage de pipelines data
- âœ… Analyse de logs d'authentification
- âœ… DÃ©tection d'anomalies simples

## ğŸ“ Licence

MIT
