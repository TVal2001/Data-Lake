"""
Spark Pipeline Complet - Orchestre ingest â†’ transform â†’ curate
"""
from pyspark.sql import SparkSession
from datetime import datetime

from spark_config import get_spark_session, stop_spark, RAW_PATH, STAGING_PATH, CURATED_PATH
from spark_ingest import ingest_category, show_raw_stats
from spark_transform import transform_category, save_to_staging
from spark_curate import (
    load_staging_data, create_login_metrics, create_user_stats,
    create_hourly_stats, create_suspicious_ips, save_to_curated
)


def run_full_pipeline():
    """
    ExÃ©cute le pipeline complet du data lake.
    
    Ã‰tapes:
    1. Ingest: Lecture des donnÃ©es raw
    2. Transform: Nettoyage et enrichissement â†’ staging
    3. Curate: AgrÃ©gation et mÃ©triques â†’ curated
    """
    print("="*60)
    print("ðŸš€ MINI DATA LAKE - PIPELINE SPARK COMPLET")
    print(f"   DÃ©marrÃ©: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    spark = get_spark_session("FullPipeline")
    
    try:
        # ========================================
        # Ã‰TAPE 1: INGEST
        # ========================================
        print("\n" + "â”"*60)
        print("ðŸ“¥ Ã‰TAPE 1: INGESTION")
        print("â”"*60)
        
        show_raw_stats(spark)
        
        # ========================================
        # Ã‰TAPE 2: TRANSFORM
        # ========================================
        print("\n" + "â”"*60)
        print("ðŸ”„ Ã‰TAPE 2: TRANSFORMATION")
        print("â”"*60)
        
        for category in ["transactions", "sales", "logs"]:
            print(f"\nâ–¶ Transformation: {category}")
            df = transform_category(spark, category)
            if df:
                save_to_staging(df, category)
                print(f"  âœ“ {df.count()} lignes transformÃ©es")
        
        # ========================================
        # Ã‰TAPE 3: CURATE
        # ========================================
        print("\n" + "â”"*60)
        print("ðŸ“¦ Ã‰TAPE 3: CURATION")
        print("â”"*60)
        
        # Traitement des logs
        df_logs = load_staging_data(spark, "logs")
        
        if df_logs:
            # MÃ©triques globales
            print("\nâ–¶ GÃ©nÃ©ration des mÃ©triques globales...")
            metrics = create_login_metrics(df_logs)
            save_to_curated(metrics, "global_metrics", "json")
            
            # Stats utilisateurs
            print("\nâ–¶ GÃ©nÃ©ration des stats utilisateurs...")
            user_stats = create_user_stats(df_logs)
            save_to_curated(user_stats, "user_statistics")
            
            # Stats horaires
            print("\nâ–¶ GÃ©nÃ©ration des stats horaires...")
            hourly = create_hourly_stats(df_logs)
            save_to_curated(hourly, "hourly_statistics")
            
            # IPs suspectes
            print("\nâ–¶ DÃ©tection des IPs suspectes...")
            suspicious = create_suspicious_ips(df_logs, threshold=5)
            save_to_curated(suspicious, "suspicious_ips")
            
            # ========================================
            # RAPPORT FINAL
            # ========================================
            print("\n" + "â”"*60)
            print("ðŸ“Š RAPPORT FINAL")
            print("â”"*60)
            
            print("\nðŸ”¹ MÃ©triques globales:")
            metrics.show(truncate=False)
            
            print("\nðŸ”¹ Top 10 utilisateurs les plus actifs:")
            user_stats.select(
                "user_id", "total_attempts", "successes", "failures", "success_rate", "activity_rank"
            ).show(10, truncate=False)
            
            print("\nðŸ”¹ ActivitÃ© par heure:")
            hourly.show(24, truncate=False)
            
            suspicious_count = suspicious.count()
            print(f"\nðŸ”¹ IPs suspectes dÃ©tectÃ©es: {suspicious_count}")
            if suspicious_count > 0:
                suspicious.show(10, truncate=False)
        
        print("\n" + "="*60)
        print(f"âœ… PIPELINE TERMINÃ‰: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
    finally:
        stop_spark(spark)


if __name__ == "__main__":
    run_full_pipeline()
